# =============================================================================
# GPU VLM SFT Configuration - Medical VQA (VQA-RAD)
# Framework: Transformers Trainer + PEFT LoRA (QLoRA optional)
# =============================================================================

model:
  # Qwen3-VL requires transformers >= 4.45.0
  # Install latest: pip install -U transformers
  name: "Qwen/Qwen3-VL-8B-Instruct"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  device_map: "auto"
  trust_remote_code: true

# Quantization (QLoRA)
quantization:
  enabled: true
  load_in_4bit: true
  load_in_8bit: false
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA config
lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  target_modules: "all-linear"
  bias: "none"

# Data paths (generated by data/scripts/prepare_medical_vqa_data.py)
data:
  train_file: "data/processed/medical_vqa/train.jsonl"
  val_file: "data/processed/medical_vqa/val.jsonl"
  max_samples: null
  max_seq_length: 2048

# Training settings
training:
  output_dir: "outputs/gpu/checkpoints/medical_vqa_sft"
  num_train_epochs: 3
  max_steps: -1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_grad_norm: 1.0
  optim: "adamw_torch"

  bf16: true
  fp16: false
  gradient_checkpointing: true

  eval_strategy: "steps"
  eval_steps: 100

  logging_steps: 10
  logging_first_step: true

  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3

  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  dataloader_num_workers: 4
  dataloader_pin_memory: true
  seed: 42

  # Optional: set when using accelerate deepspeed configs
  deepspeed: null

# Optional experiment tracking
# wandb:
#   project: "medical-vqa-gpu"
#   run_name: "qwen2-vl-7b-qlora-vqa-rad"
#   tags: ["vlm", "medical", "vqa", "sft"]

